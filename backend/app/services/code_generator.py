"""
Code Generation Service for producing executable Python code from transformation history.
Generates reproducible preprocessing pipelines using pandas and scikit-learn.
"""
import pandas as pd
from typing import Dict, Any, List, Optional
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class CodeGeneratorService:
    """Service for generating Python code from transformation history."""
    
    def __init__(self):
        """Initialize the code generator service."""
        self.logger = logging.getLogger(__name__)
    
    def generate_python_code(
        self,
        transformations: List[Dict[str, Any]],
        dataset_name: str = "data.csv",
        include_comments: bool = True,
        style: str = "pandas"  # "pandas" or "sklearn"
    ) -> str:
        """
        Generate Python code from a list of transformations.
        
        Args:
            transformations: List of transformation records
            dataset_name: Name of the input dataset file
            include_comments: Whether to include explanatory comments
            style: Code style - "pandas" for direct pandas code, "sklearn" for Pipeline
            
        Returns:
            String containing executable Python code
        """
        if style == "sklearn":
            return self._generate_sklearn_pipeline(transformations, dataset_name, include_comments)
        else:
            return self._generate_pandas_code(transformations, dataset_name, include_comments)
    
    def _generate_pandas_code(
        self,
        transformations: List[Dict[str, Any]],
        dataset_name: str,
        include_comments: bool
    ) -> str:
        """Generate pandas-based Python code."""
        
        lines = []
        
        # Header
        lines.append('"""')
        lines.append('Data Preprocessing Script')
        lines.append(f'Generated by DataPrep AI on {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        lines.append('')
        lines.append('This script reproduces the preprocessing transformations applied through the platform.')
        lines.append('"""')
        lines.append('')
        
        # Imports
        lines.append('# Required imports')
        lines.append('import pandas as pd')
        lines.append('import numpy as np')
        lines.append('')
        
        # Load data
        if include_comments:
            lines.append('# Load the dataset')
        lines.append(f'df = pd.read_csv("{dataset_name}")')
        lines.append(f'print(f"Loaded dataset: {{len(df)}} rows, {{len(df.columns)}} columns")')
        lines.append('')
        
        # Generate transformation code
        for i, transform in enumerate(transformations, 1):
            transform_type = transform.get('transformation_type', '')
            params = transform.get('parameters', {})
            columns = params.get('columns', [])
            
            if include_comments:
                lines.append(f'# Step {i}: {self._get_transformation_description(transform_type)}')
                lines.append(f'# Affected columns: {", ".join(columns) if columns else "All"}')
            
            code = self._get_transformation_code(transform_type, params)
            lines.append(code)
            lines.append('')
        
        # Summary
        lines.append('# Final dataset summary')
        lines.append('print(f"Final dataset: {len(df)} rows, {len(df.columns)} columns")')
        lines.append('print(f"Missing values: {df.isnull().sum().sum()}")')
        lines.append('')
        
        # Save
        if include_comments:
            lines.append('# Save the cleaned dataset')
        lines.append('output_file = "cleaned_" + dataset_name')
        lines.append('df.to_csv(output_file, index=False)')
        lines.append('print(f"Saved cleaned dataset to {output_file}")')
        
        return '\n'.join(lines)
    
    def _generate_sklearn_pipeline(
        self,
        transformations: List[Dict[str, Any]],
        dataset_name: str,
        include_comments: bool
    ) -> str:
        """Generate scikit-learn Pipeline code."""
        
        lines = []
        
        # Header
        lines.append('"""')
        lines.append('Data Preprocessing Pipeline (scikit-learn)')
        lines.append(f'Generated by DataPrep AI on {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        lines.append('')
        lines.append('This script creates a reusable preprocessing pipeline.')
        lines.append('"""')
        lines.append('')
        
        # Imports
        lines.append('# Required imports')
        lines.append('import pandas as pd')
        lines.append('import numpy as np')
        lines.append('from sklearn.pipeline import Pipeline')
        lines.append('from sklearn.compose import ColumnTransformer')
        lines.append('from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder')
        lines.append('from sklearn.impute import SimpleImputer')
        lines.append('import joblib')
        lines.append('')
        
        # Custom transformer class
        lines.append('# Custom transformer for operations not in sklearn')
        lines.append('from sklearn.base import BaseEstimator, TransformerMixin')
        lines.append('')
        lines.append('class CustomPreprocessor(BaseEstimator, TransformerMixin):')
        lines.append('    """Custom preprocessor that applies multiple transformations."""')
        lines.append('    ')
        lines.append('    def __init__(self, transformations=None):')
        lines.append('        self.transformations = transformations or []')
        lines.append('    ')
        lines.append('    def fit(self, X, y=None):')
        lines.append('        return self')
        lines.append('    ')
        lines.append('    def transform(self, X):')
        lines.append('        df = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)')
        lines.append('        for transform in self.transformations:')
        lines.append('            df = self._apply_transform(df, transform)')
        lines.append('        return df')
        lines.append('    ')
        lines.append('    def _apply_transform(self, df, transform):')
        lines.append('        # Apply individual transformation')
        lines.append('        transform_type = transform.get("type", "")')
        lines.append('        columns = transform.get("columns", [])')
        lines.append('        ')
        lines.append('        if transform_type == "remove_duplicates":')
        lines.append('            df = df.drop_duplicates()')
        lines.append('        elif transform_type == "drop_column":')
        lines.append('            df = df.drop(columns=columns, errors="ignore")')
        lines.append('        ')
        lines.append('        return df')
        lines.append('')
        
        # Create pipeline
        if include_comments:
            lines.append('# Define the preprocessing pipeline')
        
        lines.append('def create_preprocessing_pipeline():')
        lines.append('    """Create the preprocessing pipeline based on recorded transformations."""')
        lines.append('    ')
        lines.append('    transformations = [')
        
        for transform in transformations:
            transform_type = transform.get('transformation_type', '')
            params = transform.get('parameters', {})
            lines.append(f'        {{"type": "{transform_type}", "params": {params}}},')
        
        lines.append('    ]')
        lines.append('    ')
        lines.append('    pipeline = Pipeline([')
        lines.append('        ("custom", CustomPreprocessor(transformations)),')
        lines.append('    ])')
        lines.append('    ')
        lines.append('    return pipeline')
        lines.append('')
        
        # Main execution
        lines.append('if __name__ == "__main__":')
        lines.append(f'    # Load data')
        lines.append(f'    df = pd.read_csv("{dataset_name}")')
        lines.append('    print(f"Loaded: {len(df)} rows")')
        lines.append('    ')
        lines.append('    # Create and fit pipeline')
        lines.append('    pipeline = create_preprocessing_pipeline()')
        lines.append('    df_transformed = pipeline.fit_transform(df)')
        lines.append('    ')
        lines.append('    # Save pipeline for reuse')
        lines.append('    joblib.dump(pipeline, "preprocessing_pipeline.pkl")')
        lines.append('    print("Pipeline saved to preprocessing_pipeline.pkl")')
        lines.append('    ')
        lines.append('    # Save transformed data')
        lines.append('    df_transformed.to_csv("transformed_data.csv", index=False)')
        lines.append('    print(f"Transformed: {len(df_transformed)} rows")')
        
        return '\n'.join(lines)
    
    def _get_transformation_description(self, transform_type: str) -> str:
        """Get human-readable description for transformation type."""
        descriptions = {
            "impute_mean": "Impute missing values with mean",
            "impute_median": "Impute missing values with median",
            "impute_mode": "Impute missing values with mode",
            "impute_constant": "Impute missing values with constant",
            "impute_forward_fill": "Forward fill missing values",
            "impute_backward_fill": "Backward fill missing values",
            "remove_outliers_zscore": "Remove outliers using Z-score",
            "remove_outliers_iqr": "Remove outliers using IQR",
            "cap_outliers_zscore": "Cap outliers using Z-score",
            "cap_outliers_iqr": "Cap outliers using IQR",
            "encode_onehot": "One-hot encode categorical columns",
            "encode_label": "Label encode categorical columns",
            "convert_dtype": "Convert data type",
            "drop_column": "Drop columns",
            "remove_duplicates": "Remove duplicate rows",
            "rename_column": "Rename column",
            "scale_standard": "Standardize (z-score normalization)",
            "scale_minmax": "Min-max scaling"
        }
        return descriptions.get(transform_type, transform_type)
    
    def _get_transformation_code(self, transform_type: str, params: Dict[str, Any]) -> str:
        """Get pandas code for a specific transformation."""
        columns = params.get('columns', [])
        col_list = ', '.join([f'"{c}"' for c in columns])
        
        code_templates = {
            "impute_mean": f'for col in [{col_list}]:\n    df[col].fillna(df[col].mean(), inplace=True)',
            "impute_median": f'for col in [{col_list}]:\n    df[col].fillna(df[col].median(), inplace=True)',
            "impute_mode": f'for col in [{col_list}]:\n    mode_val = df[col].mode()\n    if len(mode_val) > 0:\n        df[col].fillna(mode_val[0], inplace=True)',
            "impute_constant": f'for col in [{col_list}]:\n    df[col].fillna({params.get("constant_value", "None")}, inplace=True)',
            "impute_forward_fill": f'for col in [{col_list}]:\n    df[col].ffill(inplace=True)',
            "impute_backward_fill": f'for col in [{col_list}]:\n    df[col].bfill(inplace=True)',
            "remove_outliers_zscore": f'''threshold = {params.get("threshold", 3.0)}
for col in [{col_list}]:
    z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
    df = df[z_scores <= threshold]
df.reset_index(drop=True, inplace=True)''',
            "remove_outliers_iqr": f'''iqr_multiplier = {params.get("iqr_multiplier", 1.5)}
for col in [{col_list}]:
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    df = df[(df[col] >= q1 - iqr_multiplier * iqr) & (df[col] <= q3 + iqr_multiplier * iqr)]
df.reset_index(drop=True, inplace=True)''',
            "cap_outliers_zscore": f'''threshold = {params.get("threshold", 3.0)}
for col in [{col_list}]:
    mean, std = df[col].mean(), df[col].std()
    df[col] = df[col].clip(lower=mean - threshold * std, upper=mean + threshold * std)''',
            "cap_outliers_iqr": f'''iqr_multiplier = {params.get("iqr_multiplier", 1.5)}
for col in [{col_list}]:
    q1, q3 = df[col].quantile(0.25), df[col].quantile(0.75)
    iqr = q3 - q1
    df[col] = df[col].clip(lower=q1 - iqr_multiplier * iqr, upper=q3 + iqr_multiplier * iqr)''',
            "encode_onehot": f'''for col in [{col_list}]:
    dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)
    df = df.drop(columns=[col])
    df = pd.concat([df, dummies], axis=1)''',
            "encode_label": f'''for col in [{col_list}]:
    df[col] = df[col].astype("category").cat.codes''',
            "convert_dtype": f'''target_dtype = "{params.get("target_dtype", "float")}"
for col in [{col_list}]:
    if target_dtype == "int":
        df[col] = pd.to_numeric(df[col], errors="coerce").astype("Int64")
    elif target_dtype == "float":
        df[col] = pd.to_numeric(df[col], errors="coerce")
    elif target_dtype == "str":
        df[col] = df[col].astype(str)
    elif target_dtype == "datetime":
        df[col] = pd.to_datetime(df[col], errors="coerce")''',
            "drop_column": f'df.drop(columns=[{col_list}], inplace=True)',
            "remove_duplicates": 'df.drop_duplicates(inplace=True)\ndf.reset_index(drop=True, inplace=True)',
            "rename_column": f'df.rename(columns={{"{columns[0] if columns else "old_name"}": "{params.get("new_name", "new_name")}"}}, inplace=True)',
            "scale_standard": f'''for col in [{col_list}]:
    mean, std = df[col].mean(), df[col].std()
    if std > 0:
        df[col] = (df[col] - mean) / std''',
            "scale_minmax": f'''for col in [{col_list}]:
    min_val, max_val = df[col].min(), df[col].max()
    if max_val > min_val:
        df[col] = (df[col] - min_val) / (max_val - min_val)'''
        }
        
        return code_templates.get(transform_type, f'# TODO: Implement {transform_type}')
    
    def generate_notebook(
        self,
        transformations: List[Dict[str, Any]],
        dataset_name: str = "data.csv"
    ) -> Dict[str, Any]:
        """
        Generate a Jupyter notebook structure.
        
        Args:
            transformations: List of transformation records
            dataset_name: Name of the input dataset file
            
        Returns:
            Dict in Jupyter notebook format
        """
        cells = []
        
        # Title cell
        cells.append({
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Preprocessing Notebook\n",
                f"Generated by DataPrep AI on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
                "\n",
                "This notebook reproduces the preprocessing steps applied through the platform."
            ]
        })
        
        # Imports cell
        cells.append({
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline"
            ],
            "execution_count": None,
            "outputs": []
        })
        
        # Load data cell
        cells.append({
            "cell_type": "code",
            "metadata": {},
            "source": [
                f'df = pd.read_csv("{dataset_name}")\n',
                'print(f"Dataset shape: {df.shape}")\n',
                'df.head()'
            ],
            "execution_count": None,
            "outputs": []
        })
        
        # Transformation cells
        for i, transform in enumerate(transformations, 1):
            transform_type = transform.get('transformation_type', '')
            params = transform.get('parameters', {})
            
            cells.append({
                "cell_type": "markdown",
                "metadata": {},
                "source": [f"## Step {i}: {self._get_transformation_description(transform_type)}"]
            })
            
            cells.append({
                "cell_type": "code",
                "metadata": {},
                "source": [self._get_transformation_code(transform_type, params)],
                "execution_count": None,
                "outputs": []
            })
        
        # Summary cell
        cells.append({
            "cell_type": "code",
            "metadata": {},
            "source": [
                '# Final summary\n',
                'print(f"Final shape: {df.shape}")\n',
                'print(f"Missing values: {df.isnull().sum().sum()}")\n',
                'df.info()'
            ],
            "execution_count": None,
            "outputs": []
        })
        
        return {
            "nbformat": 4,
            "nbformat_minor": 4,
            "metadata": {
                "kernelspec": {
                    "display_name": "Python 3",
                    "language": "python",
                    "name": "python3"
                }
            },
            "cells": cells
        }


# Global instance
code_generator = CodeGeneratorService()
